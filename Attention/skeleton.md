# Self Attention based approach to Coreference Resolution

### Things to be done
1. Read the papers 
   1. [Multi-Level Attention Based Coreference Resolution With Gated Recurrent Unit and Convolutional Neural Networks](https://ieeexplore.ieee.org/abstract/document/10006801)
   2. [Multi-task learning with contextual hierarchical attention for Korean coreference resolution](https://onlinelibrary.wiley.com/doi/full/10.4218/etrij.2021-0293)
   4. [Tackling Zero Pronoun Resolution and Non-Zero Coreference Resolution Jointly](https://aclanthology.org/2021.conll-1.40/)
2. Make a note of datasets and methods/models used
   1. Datasets
      1. MUC 
      2. B<sup>3</sup>
      3. CEAF
      4. OntoNotes 5.0 Chinese corpus
   2. Models/Methods
      1. LSTM 
      2. Att-GRU-CNN based on multi-level attention
      3. Hierarchical encoder-decoder (HRED) model with Multitask Learning
      4. MatchingAware Attention Kernel (MAAK)
      5. Dual Attention Networks (DAN)
      6. A gapmasked self-attention model
3. Make up something of our own / Add a functionality of your own <br>
  The idea here is to possibly use a mix of 2-3 papers above mentioned and get better results than state-of-art 
4. Figure the rest out :) !!!